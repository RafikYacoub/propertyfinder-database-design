{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41fb4514",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-28 21:40:44.587358\n",
      "310\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 393\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperty-amenities__list\u001b[39m\u001b[38;5;124m\"\u001b[39m}):\n\u001b[0;32m    392\u001b[0m         amenities \u001b[38;5;241m=\u001b[39m amenities \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m--> 393\u001b[0m     amenities \u001b[38;5;241m=\u001b[39m \u001b[43mamenities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m          \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    394\u001b[0m     amenities \u001b[38;5;241m=\u001b[39m amenities\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import requests\n",
    "import json\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from random import randint\n",
    "\n",
    "#headers\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.42'}\n",
    "\n",
    "\n",
    "\n",
    "page = 1\n",
    "\n",
    "\n",
    "#generating fake user data \n",
    "\n",
    "if (page == 1):\n",
    "    gender = ['M','F']\n",
    "    l=[]\n",
    "\n",
    "    emails=[]\n",
    "\n",
    "    Faker.seed(32)\n",
    "    fake = Faker(locale='en_US')\n",
    "\n",
    "    for x in range(0,10):\n",
    "        e=fake.email()\n",
    "        e=\"'\"+e+\"'\"\n",
    "        emails.append(e)\n",
    "        g=random.choice(gender)\n",
    "        if (g == 'M'):\n",
    "            n=fake.name_male()\n",
    "            n=\"'\"+n+\"'\"\n",
    "        else:\n",
    "            n=fake.name_female()\n",
    "            n=\"'\"+n+\"'\"\n",
    "        g=\"'\"+g+\"'\"\n",
    "        u=fake.user_name()\n",
    "        u=\"'\"+u+\"'\"\n",
    "        d=fake.date_between_dates(date_start=datetime(1950,1,1), date_end=datetime(2000,10,20))\n",
    "        #d=\"'\"+str(d)+\"'\"\n",
    "        #d=str(d)\n",
    "        ad=fake.address()\n",
    "        ad = ad.replace(\"\\n\",\" \")\n",
    "        ad = ad.replace(\",\",\" ;\")\n",
    "        ad=\"'\"+ad+\"'\"\n",
    "        l.append([u,n,g,ad,e,d]) \n",
    "\n",
    "\n",
    "\n",
    "    df7 = pd.DataFrame(l)\n",
    "    df7.to_csv(\"user_test.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=False)\n",
    "\n",
    "    l2=[]\n",
    "\n",
    "    focus_areas = ['Sheikh Zayed', 'Giza','Heliopolis']\n",
    "    focus_areas2 = ['New Cairo','Nasr City']\n",
    "    for x in range(0,5):\n",
    "        l2.append([emails[x],\"'\"+random.choice(focus_areas)+\"'\"])\n",
    "        l2.append([emails[x],\"'\"+random.choice(focus_areas2)+\"'\"])\n",
    "\n",
    "    df8 = pd.DataFrame(l2)\n",
    "    df8.to_csv(\"user_focus_areas.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "#lists to hold the type for the properties\n",
    "IDS = []\n",
    "TYPES = []\n",
    "AREAS = []\n",
    "BEDROOMS = []\n",
    "BATHROOMS = []\n",
    "LISTED_DATES= []\n",
    "PRICES = []\n",
    "LOCATIONS = []\n",
    "DESCRIPTIONS = []\n",
    "AMENITIES = []\n",
    "AGENTS_NUMBERS = []\n",
    "\n",
    "#for agent table\n",
    "AGENTS = []\n",
    "COMPANIES = []\n",
    "\n",
    "#lists to hold the type for the devlopment projects\n",
    "DEVELOPMENT_PROJECTS_NAMES = []\n",
    "DEVELOPERS = []\n",
    "PROJECTS_LOCATIONS = []\n",
    "PROJECT_TYPES = []\n",
    "PROJECT_STARTING_PRICES =[]\n",
    "PROJECT_SIZE_MIN =[]\n",
    "PROJECT_SIZE_MAX =[]\n",
    "PROJECT_STATUS =[]\n",
    "\n",
    "\n",
    "#lists to use the ids, locations, and areas and phone numbers of agents \n",
    "#of the first page properties to assign them random to feedback and shortlisted tables\n",
    "ID2=[]\n",
    "location2=[]\n",
    "area2=[]\n",
    "numbers2=[]\n",
    "\n",
    "\n",
    "\n",
    "#scraping the property data by going through the search page(sorted by the newest)\n",
    "#then opening each search result(25 per page)\n",
    "#then going to the next page through manipulating index in the url\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print(now)\n",
    "\n",
    "\n",
    "for x in range(page,312):\n",
    "    #print(\"page: \", page)\n",
    "    #search page and its reponse\n",
    "    search_page = \"https://www.propertyfinder.eg/en/search?c=1&ob=nd&page=\"+str(x)+\"&pt=3000000\"\n",
    "    search_response = requests.get(search_page,headers=HEADERS)\n",
    "    soup_search = BeautifulSoup(search_response.text,'html.parser')\n",
    "    \n",
    "    #list to hold the search results links(25 property per page)\n",
    "    \n",
    "    urls=[]\n",
    "    \n",
    "    #manipulating the links to filter them to get only links for the 25 properties\n",
    "    \n",
    "    for link in soup_search.find_all(\"a\"):\n",
    "        urls.append(\"https://www.propertyfinder.eg\"+link.get('href'))\n",
    "        \n",
    "    del urls[0:32]\n",
    "    lg=len(urls)\n",
    "    del urls[25: lg]\n",
    "    lg=len(urls)\n",
    "    \n",
    "    responses=[]\n",
    "    soups=[]\n",
    "    i=0\n",
    "    #traversing through the 25 links we got from each search page to scrape\n",
    "    #using webdriver to open the each page and click on the call button \n",
    "    #to get the updated HTML with the agent's phone number\n",
    "    \n",
    "    for url in urls:\n",
    "        res = requests.get(url,headers=HEADERS)\n",
    "        responses.append(res)\n",
    "        soups.append(BeautifulSoup(responses[i].text,'html.parser'))\n",
    "        i = i+1    \n",
    "    \n",
    "    #traversing through the soup of each property of the 25 properties per page\n",
    "   \n",
    "    for soup in soups:\n",
    "        \n",
    "        \n",
    "        \n",
    "        #getting the property ID and appending it to its list\n",
    "        \n",
    "        reference_no = soup.find_all(\"div\", {\"class\": \"property-page__legal-list-content\"})[0].text\n",
    "        reference_no = \"\\'\"+reference_no+\"\\'\"\n",
    "        if (page == 1):\n",
    "            ID2.append(reference_no)\n",
    "        IDS.append(reference_no)\n",
    "        #print(reference_no)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #getting the agent's phone number and appending it to its list\n",
    "        \n",
    "        #agent_phone_no = soup.find_all(\"a\",{\"class\" : \"text text--size4 text--bold property-contact__number\"})[0].text\n",
    "        ser=soup.find_all(\"script\" )\n",
    "        x=ser[len(ser)-3].string\n",
    "        text = x[x.index(\"window.propertyfinder.settings.property =\")+len(\"window.propertyfinder.settings.property =\"):x.index(\"window.propertyfinder.settings.search\")]\n",
    "        text = text[text.index('payload:')+len('payload:'):text.rfind(';')]\n",
    "        text = text[0: text.rfind(',')]\n",
    "        w = json.loads(text)\n",
    "        agent_phone_no = w['data']['meta']['contact_options']['list']['phone']['value']\n",
    "        agent_phone_no = agent_phone_no.replace(\"+2\", \"\")\n",
    "        lo=\"\\'\"+agent_phone_no+\"\\'\"\n",
    "        AGENTS_NUMBERS.append(lo)\n",
    "        if (page == 1):\n",
    "            numbers2.append(lo)\n",
    "        #print(agent_phone_no)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #getting the agent's broker company and appending it to its list\n",
    "        \n",
    "        br = soup.find_all(\"div\",{\"class\" : \"property-agent__position-broker-name\"})[0].text.split('\\n')\n",
    "        br = br[1].split()\n",
    "        broker_company_name=\"\"\n",
    "        for x in br:\n",
    "            broker_company_name = broker_company_name + ' ' + x\n",
    "        COMPANIES.append(broker_company_name)\n",
    "        #print(broker_company)\n",
    "        \n",
    "        \n",
    "        #getting the number of attribute in the following div to check if the number\n",
    "        #of bedrooms and bathrooms is mentioned (attr number > 2 or 3) or not \n",
    "        #(attr num) < 2 to avoid accessing element of the div out of the index\n",
    "        \n",
    "        no_of_attr = len(soup.find_all(\"div\", {\"class\": \"property-facts__value\"}))\n",
    "        #print(\"no of attr: \", no_of_attr)\n",
    "        \n",
    "        #getting the property type and appending it to its list\n",
    "        \n",
    "        if (no_of_attr>0):\n",
    "            type = soup.find_all(\"div\", {\"class\": \"property-facts__value\"})[0].text.split() \n",
    "            if (len(type)>1):\n",
    "                type = type[0] + ' ' + type [1]\n",
    "            else:\n",
    "                type = type[0]\n",
    "            type = \"\\'\"+type+\"\\'\"\n",
    "        else:\n",
    "            type = None\n",
    "        TYPES.append(type)\n",
    "            #print(type)\n",
    "        \n",
    "        \n",
    "        #getting the property area and appending it to its list\n",
    "        if (no_of_attr>1):\n",
    "            ar = soup.find_all(\"div\", {\"class\": \"property-facts__value\"})[1].text.split()\n",
    "            if (len(ar)>2):\n",
    "                area = ar[3]\n",
    "                area = area.replace(\",\",\"\")\n",
    "                area = int(area)\n",
    "            else:\n",
    "                area = 0  \n",
    "        else:\n",
    "            area = 0\n",
    "        if (page == 1):\n",
    "            area2.append(area)\n",
    "        AREAS.append(area)\n",
    "            #print(area)\n",
    "        \n",
    "        \n",
    "        #getting the property number of bedrooms and appending it to its list\n",
    "        \n",
    "        if (no_of_attr>2):\n",
    "            bedrooms_no = soup.find_all(\"div\", {\"class\": \"property-facts__value\"})[2].text.split()\n",
    "            if (len(bedrooms_no)>1 and bedrooms_no[0].isdecimal() == True):\n",
    "                bedrooms_no = int(bedrooms_no[0])+1 #check if there's a maid room, if yes, then the number + 1\n",
    "            else:\n",
    "                if(bedrooms_no[0].isdecimal() == False):\n",
    "                    bedrooms_no = 1\n",
    "                else:\n",
    "                    bedrooms_no = int(bedrooms_no[0])\n",
    "        else:\n",
    "            bedrooms_no = 0 \n",
    "        \n",
    "        BEDROOMS.append(bedrooms_no)\n",
    "            #print(bedrooms_no)\n",
    "            \n",
    "            \n",
    "        #getting the property number of bathrooms and appending it to its list\n",
    "        \n",
    "        if (no_of_attr>3):\n",
    "            bathrooms_no = soup.find_all(\"div\", {\"class\": \"property-facts__value\"})[3].text.split()\n",
    "            if(bathrooms_no[0].isdecimal() == False):\n",
    "                bathrooms_no = bathrooms_no[0]\n",
    "                bathrooms_no = bathrooms_no.replace(\"+\",\"\")\n",
    "                bathrooms_no = int(bathrooms_no[0]) + 1\n",
    "            else:\n",
    "                bathrooms_no = int(bathrooms_no[0])\n",
    "            \n",
    "        else:\n",
    "            bathrooms_no = 0 \n",
    "        \n",
    "        BATHROOMS.append(bathrooms_no)\n",
    "            #print(bathrooms_no)\n",
    "        \n",
    "                \n",
    "        \n",
    "        #getting the property price and appending it to its list\n",
    "        \n",
    "        pr = soup.find_all(\"div\", {\"class\": \"property-price__price\"})[0].text.split()\n",
    "        pr = pr[0].split(',')\n",
    "        price=\"\"\n",
    "        for x in pr:\n",
    "            price = price  + x\n",
    "        \n",
    "        if (price.isdecimal() == False):\n",
    "            price=0          #+ ' '+ \"EGP\"\n",
    "        else:\n",
    "            price = int(price)\n",
    "        \n",
    "        \n",
    "        PRICES.append(price)\n",
    "        #print(price)\n",
    "        \n",
    "        \n",
    "        #getting the property location and appending it to its list\n",
    "        \n",
    "        location = soup.find_all(\"div\",{\"class\" : \"text text--size3 property-location__tower-name\"})[0].text  \n",
    "        location = location + ' ' + soup.find_all(\"div\",{\"class\" : \"text text--size3\"})[0].text\n",
    "        location = location.replace(\",\", \" \" )\n",
    "        location = \"'\" + location + \"'\"\n",
    "        if (page ==1):\n",
    "            location2.append(location)\n",
    "        LOCATIONS.append(location)\n",
    "        #print(location)\n",
    "        \n",
    "        \n",
    "        #getting the agent's name and appending it to its list\n",
    "        \n",
    "        agent_name = soup.find_all(\"h4\",{\"class\" : \"text text--size3 property-agent__name\"})[0].text\n",
    "        AGENTS.append(agent_name)\n",
    "        #print(agent_name)\n",
    "        \n",
    "        \n",
    "        #getting the broker's name and appending it to its list\n",
    "        \n",
    "        broker_company = soup.find_all(\"div\",{\"class\" : \"property-agent__position-broker-name\"})[0].text.split('\\n')\n",
    "        broker_company = broker_company[1].split()\n",
    "        broker_company = \" \".join(broker_company)\n",
    "        broker_company = '\\''+broker_company+'\\''\n",
    "        COMPANIES.append(broker_company_name)\n",
    "        #print(broker_company)\n",
    "        \n",
    "        \n",
    "        #getting the property description and appending it to its list\n",
    "        \n",
    "        description = soup.find_all(\"div\",{\"data-qs\":\"text-trimmer\"})[0].text\n",
    "        description = description.replace(\"\\n\",\" \")\n",
    "        description = description.replace(\",\",\" ;\")\n",
    "        DESCRIPTIONS.append(description)\n",
    "        #print(description)\n",
    "        \n",
    "        \n",
    "        #checking if the property has amentities section and if so, getting the property amenities and appending it to its list\n",
    "        \n",
    "        if len(soup.find_all(\"div\",{\"class\" : \"property-amenities__list\"})) != 0: \n",
    "            amenities = \"\"\n",
    "            for x in soup.find_all(\"div\",{\"class\" : \"property-amenities__list\"}):\n",
    "                amenities = amenities + x.text\n",
    "            amenities = amenities.split('          ')[2]\n",
    "            amenities = amenities.split('\\n')\n",
    "            \n",
    "        else:\n",
    "            amenities = \"NULL\"\n",
    "        AMENITIES.append(amenities)\n",
    "        #print(amenities)\n",
    "        \n",
    "        \n",
    "        #checking if the property is part of development project and if so,\n",
    "        #getting the project name and appending it to its list\n",
    "\n",
    "       \n",
    " \n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__title\"})) > 0:\n",
    "            project_name =soup.find_all(\"div\", {\"class\": \"property-project-details__title\"})[0].text\n",
    "            project_name = \"'\"+project_name+\"'\"    \n",
    "            #print(project_name)\n",
    "        else:\n",
    "            project_name = \"NULL\"\n",
    "\n",
    "        DEVELOPMENT_PROJECTS_NAMES.append(project_name)\n",
    "\n",
    "        \n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__location\"})) > 0:\n",
    "            project_location =soup.find_all(\"div\", {\"class\": \"property-project-details__location\"})[0].text\n",
    "            project_location = project_location.replace(\",\",\"; \")\n",
    "            #print(project_location)\n",
    "        else:\n",
    "            project_location = \"NULL\"\n",
    "\n",
    "\n",
    "        PROJECTS_LOCATIONS.append(project_location)\n",
    "        \n",
    "\n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__developer-box-developed-by\"})) > 0:\n",
    "            developer =soup.find_all(\"div\", {\"class\": \"property-project-details__developer-box-developed-by\"})[0].next_element.next_element.next_element.text\n",
    "            developer = \"'\"+developer+\"'\"  \n",
    "            #print(developer)\n",
    "        else:\n",
    "            developer = \"NULL\"\n",
    "\n",
    "        DEVELOPERS.append(developer)\n",
    "\n",
    "\n",
    "\n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})) > 0:\n",
    "            pr =soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})[0].text.split()\n",
    "            project_types=[]\n",
    "            for x in pr:\n",
    "                if (x!=\",\"):\n",
    "                    x=\"'\"+x+\"'\"\n",
    "                    project_types.append(x)\n",
    "\n",
    "            #print(project_types)\n",
    "        else:\n",
    "            project_types = \"NULL\"\n",
    "\n",
    "        PROJECT_TYPES.append(project_types)\n",
    "\n",
    "\n",
    "\n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})) > 1:\n",
    "            t = soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})[1].text.split()\n",
    "            project_size_range = []\n",
    "            for x in t:\n",
    "                if(x!=\"-\"):\n",
    "                    project_size_range.append(x)\n",
    "            min_size=int(project_size_range[0])\n",
    "            max_size=int(project_size_range[1])\n",
    "        else:\n",
    "            min_size=0\n",
    "            max_size=0\n",
    "        \n",
    "        PROJECT_SIZE_MIN.append(min_size)\n",
    "        PROJECT_SIZE_MAX.append(max_size)\n",
    "\n",
    "\n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})) > 2:\n",
    "            t =soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})[2].text.split()\n",
    "            t = t[0]\n",
    "            t = t.split(',')\n",
    "            project_starting_price = \"\"\n",
    "            for x in t:\n",
    "                project_starting_price = project_starting_price + x\n",
    "                \n",
    "            if (project_starting_price.isdecimal() == False):\n",
    "                project_starting_price=0\n",
    "            else:\n",
    "                project_starting_price = int(project_starting_price)\n",
    "            #print(project_starting_price)\n",
    "        else:\n",
    "            project_starting_price = 0\n",
    "        \n",
    "        PROJECT_STARTING_PRICES.append(project_starting_price)\n",
    "\n",
    "\n",
    "\n",
    "        if len(soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})) > 3:\n",
    "            ps =soup.find_all(\"div\", {\"class\": \"property-project-details__list-item-value\"})[3].text.split()\n",
    "            project_status=\"\"\n",
    "            for x in ps:\n",
    "                project_status = project_status +x\n",
    "            project_status=\"'\"+project_status+\"'\"\n",
    "            #print(project_status)\n",
    "        else:\n",
    "            project_status = \"NULL\"\n",
    "        \n",
    "        PROJECT_STATUS.append(project_status)\n",
    "        \n",
    "       \n",
    "\n",
    "    del responses\n",
    "    del urls\n",
    "    del soups\n",
    "    \n",
    "    \n",
    "    #to replace the none value in the project names with null to be accepted in mysql\n",
    "    \n",
    "    \n",
    "    #creating list to concatenate the attributes for each table then creating a dataframe out of them and exporting it as csv file\n",
    "    \n",
    "    \n",
    "    prop=[]\n",
    "    for x in range(0,len(IDS)):\n",
    "        dupli = False\n",
    "        for y in range(0,x):\n",
    "            if ((IDS[x] == IDS[y] and AREAS[x] == AREAS[y] and LOCATIONS[x] == LOCATIONS[y])or (AREAS[x] ==0)):\n",
    "                dupli = True\n",
    "        if(dupli == False):\n",
    "            prop.append([IDS[x],AREAS[x],LOCATIONS[x], TYPES[x], BEDROOMS[x], BATHROOMS[x],PRICES[x],AGENTS_NUMBERS[x],DEVELOPMENT_PROJECTS_NAMES[x],\"\\'\"+DESCRIPTIONS[x]+\"\\'\"]) \n",
    "    df1 = pd.DataFrame(prop)\n",
    "    df1.to_csv(\"prop_test.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    amen=[]\n",
    "    for x in range(0,len(IDS)):\n",
    "        dupli = False\n",
    "        for y in range(0,x):\n",
    "            if ((IDS[x] == IDS[y] and AREAS[x] == AREAS[y] and LOCATIONS[x] == LOCATIONS[y])or (AREAS[x] ==0)):\n",
    "                dupli = True\n",
    "        if(dupli == False):\n",
    "            if (AMENITIES[x] != \"NULL\"):\n",
    "                for w in range(0,len(AMENITIES[x])):\n",
    "                    amen.append([IDS[x],LOCATIONS[x],AREAS[x],AMENITIES[x][w]]) \n",
    "        \n",
    "    df2 = pd.DataFrame(amen)\n",
    "    df2.to_csv(\"prop_amen.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    ag=[]\n",
    "    for x in range(0,len(AGENTS_NUMBERS)):\n",
    "        dupli = False\n",
    "        for y in range(0,x): \n",
    "            if (AGENTS_NUMBERS[x] == AGENTS_NUMBERS[y]):\n",
    "                dupli = True\n",
    "        if(dupli == False):\n",
    "            ag.append([AGENTS[x],AGENTS_NUMBERS[x]]) \n",
    "        \n",
    "    df3 = pd.DataFrame(ag)\n",
    "    df3.to_csv(\"agents.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    work_for=[]\n",
    "    for x in range(0,len(AGENTS_NUMBERS)):\n",
    "        dupli = False\n",
    "        for y in range(0,x): \n",
    "            if (AGENTS_NUMBERS[x] == AGENTS_NUMBERS[y] and COMPANIES[x] == COMPANIES[y]):\n",
    "                dupli = True\n",
    "        if(dupli == False):\n",
    "            l = COMPANIES[x].split()\n",
    "            l =\" \".join(l)\n",
    "            l = '\\''+l+'\\''\n",
    "            work_for.append([AGENTS_NUMBERS[x],l]) \n",
    "        \n",
    "    df4 = pd.DataFrame(work_for)\n",
    "    df4.to_csv(\"work_for.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    dev_proj=[]\n",
    "    for x in range(0,len(DEVELOPMENT_PROJECTS_NAMES)):\n",
    "        dupli = False\n",
    "        for y in range(0,x): \n",
    "            if (DEVELOPMENT_PROJECTS_NAMES[x] == DEVELOPMENT_PROJECTS_NAMES[y] and DEVELOPMENT_PROJECTS_NAMES[x] != None):\n",
    "                dupli = True\n",
    "        if(dupli == False and x!=0):\n",
    "            dev_proj.append([DEVELOPMENT_PROJECTS_NAMES[x],DEVELOPERS[x],PROJECTS_LOCATIONS[x],PROJECT_STARTING_PRICES[x],PROJECT_SIZE_MIN[x],PROJECT_SIZE_MAX[x],PROJECT_STATUS[x]]) \n",
    "        \n",
    "    df5 = pd.DataFrame(dev_proj)\n",
    "    df5.to_csv(\"dev_proj.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    proj_types=[]\n",
    "    for x in range(0,len(DEVELOPMENT_PROJECTS_NAMES)):\n",
    "        dupli = False\n",
    "        for y in range(0,x):\n",
    "            if (DEVELOPMENT_PROJECTS_NAMES[x] == DEVELOPMENT_PROJECTS_NAMES[y] and DEVELOPMENT_PROJECTS_NAMES[x] != None):\n",
    "                dupli = True\n",
    "        if(dupli == False):\n",
    "            if (PROJECT_TYPES[x] != \"NULL\"):\n",
    "                for w in range(0,len(PROJECT_TYPES[x])):\n",
    "                    proj_types.append([DEVELOPMENT_PROJECTS_NAMES[x],PROJECT_TYPES[x][w]]) \n",
    "        \n",
    "    df6 = pd.DataFrame(proj_types)\n",
    "    df6.to_csv(\"proj_types.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #clearing the list to save memory after each page \n",
    "    \n",
    "    IDS.clear()\n",
    "    TYPES.clear()\n",
    "    AREAS.clear()\n",
    "    BEDROOMS.clear()\n",
    "    BATHROOMS.clear()\n",
    "    LISTED_DATES.clear()\n",
    "    PRICES.clear()\n",
    "    LOCATIONS.clear()\n",
    "    AGENTS.clear()\n",
    "    COMPANIES.clear()\n",
    "    DESCRIPTIONS.clear()\n",
    "    AMENITIES.clear()\n",
    "    AGENTS_NUMBERS.clear()\n",
    "    AGENTS.clear()\n",
    "    COMPANIES.clear()\n",
    "    DEVELOPMENT_PROJECTS_NAMES.clear()\n",
    "    DEVELOPERS.clear()\n",
    "    PROJECTS_LOCATIONS.clear()\n",
    "    PROJECT_TYPES.clear()\n",
    "    PROJECT_STARTING_PRICES.clear()\n",
    "    PROJECT_SIZE_MIN.clear()\n",
    "    PROJECT_SIZE_MAX.clear()\n",
    "    PROJECT_STATUS.clear()\n",
    "\n",
    "    \n",
    "    #creating the fake data needed for shortlisting properties and giving feedbacks to agents\n",
    "    \n",
    "    if (page == 1):\n",
    "\n",
    "        l3=[]\n",
    "\n",
    "        for x in range(0,5):\n",
    "            for y in range(x,x+2):\n",
    "                l3.append([ID2[x],location2[x],area2[x],emails[y]])\n",
    "\n",
    "        df9 = pd.DataFrame(l3)\n",
    "        df9.to_csv(\"shortlisted.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def random_with_N_digits(n):\n",
    "            range_start = 10**(n-1)\n",
    "            range_end = (10**n)-1\n",
    "            return randint(range_start, range_end)\n",
    "\n",
    "        Faker.seed(13)\n",
    "\n",
    "        fake= Faker(locale='en_US')\n",
    "        fake_feedback=[]\n",
    "        for x in range(10):\n",
    "            fake_feedback.append([numbers2[x],emails[x],fake.date_between_dates(date_start=datetime(2022,10,1), date_end=datetime(2022,10,20)),random_with_N_digits(1),fake.paragraph()])\n",
    "\n",
    "        df10 = pd.DataFrame(fake_feedback)\n",
    "        df10.to_csv(\"feedback.csv\", mode=\"a\",sep=',', encoding='utf-8', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print(page)\n",
    "    page = page +1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1e32943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-28 11:45:31.739938\n",
      "New Avenue Real Estate\n",
      "Adviser Real Estate\n",
      "Insider Real Estate Consultancy\n",
      "TAI - The Address Investments\n",
      "GPR Property\n",
      "RE/MAX Avalon\n",
      "Egypt Best Properties\n",
      "The Deal Real Estate\n",
      "Egypt Best Properties West\n",
      "URE-Ultimate Real Estate\n",
      "CBE New Homes\n",
      "IRTKAZ\n",
      "Green Homes Real estate Agency\n",
      "Evara\n",
      "T4P\n",
      "Remax The Address\n",
      "Elshams Real Estate\n",
      "IMKAN Investments\n",
      "37\n",
      "Step One Real Estate\n",
      "Pro Max for Real Estate\n",
      "Sedra Real Estate\n",
      "Next Door Consultancy\n",
      "Go Green Egypt Real Estate\n",
      "BRIXWELL PROPERTIES\n",
      "Invest Gate\n",
      "Empire real estate and investment\n",
      "Abrag Real Estate\n",
      "Properties-Today.com\n",
      "C Zone Real Estate\n",
      "Integrated Realtors Group for Real Estate\n",
      "RE/MAX Al Mohager\n",
      "Al Maz Real Estate\n",
      "Snap Home Real Estate\n",
      "Point Mark For Real Estate\n",
      "Zayed Homes\n",
      "Allocate Real Estate\n",
      "38\n",
      "KAB For Real Estate\n",
      "Investory Real Estate\n",
      "RE/MAX Re ADVISOR\n",
      "Concept Egypt\n",
      "Season SR\n",
      "Roots Development\n",
      "Onyx for real estate\n",
      "شركه مسك\n",
      "Zain Real Estate Investment\n",
      "Connect Homes Real Estate\n",
      "ElGammal real estate\n",
      "Home's\n",
      "Prime Consultancy\n",
      "Real Estate Corner\n",
      "YaKan Properties\n",
      "Afnan Real Estate\n",
      "Better Life Real Estate\n",
      "Heart Of The Capital for Resale\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'company_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m         company_data\u001b[38;5;241m.\u001b[39mappend([COMPANY_NAMES[x],COMPANY_NUMBERS[x],ACTIVE_LISTING[x],COMPANY_ADDRESS[x],COMPANY_ABOUT[x]])\n\u001b[0;32m     95\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(company_data)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompany_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(page)\n\u001b[0;32m     99\u001b[0m page \u001b[38;5;241m=\u001b[39m page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3721\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3710\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3712\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3713\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3714\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3718\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3719\u001b[0m )\n\u001b[1;32m-> 3721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'company_test.csv'"
     ]
    }
   ],
   "source": [
    "#lists to hold the type for the broker companies\n",
    "COMPANY_NAMES = []\n",
    "ACTIVE_LISTING = []\n",
    "COMPANY_ABOUT = []\n",
    "COMPANY_ADDRESS = []\n",
    "COMPANY_NUMBERS =[]\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "page =37\n",
    "for x in range(1,page):\n",
    "    \n",
    "    broker_page = \"https://www.propertyfinder.eg/en/find-broker/search?page=\"+ str(x)\n",
    "    broker_response = requests.get(broker_page,headers=HEADERS)\n",
    "    soup_broker = BeautifulSoup(broker_response.text,'html.parser')\n",
    "    urls=[]\n",
    "    \n",
    "    for link in soup_broker.findAll('a'):\n",
    "        urls.append(\"https://www.propertyfinder.eg\"+link.get('href'))\n",
    "\n",
    "    del urls[0:16]\n",
    "    lg=len(urls)\n",
    "    del urls[18: lg]\n",
    "\n",
    "\n",
    "    \n",
    "    responses=[]\n",
    "    soups=[]\n",
    "    i=0\n",
    "    for url in urls:\n",
    "        res=requests.get(url,headers=HEADERS)\n",
    "        responses.append(res.text)\n",
    "        soups.append(BeautifulSoup(responses[i],'html.parser'))\n",
    "        i = i+1\n",
    "    \n",
    "    \n",
    "    for soup in soups:\n",
    "        company_name = soup.find_all(\"h1\",{\"class\":\"title title--size1 title--bottom-space2 bio-info__name\"})[0].text\n",
    "        company_name =\"'\"+ company_name+\"'\"\n",
    "        COMPANY_NAMES.append(company_name)\n",
    "        print(company_name) \n",
    "        \n",
    "        active_listing = soup.find_all(\"a\",{\"class\":\"link\"},{\"href\":\"#tab-properties\"})[1].text.split()\n",
    "        al= active_listing[0].split(',')\n",
    "        if (len(al)>1):\n",
    "            active_listing=\"\"\n",
    "            for x in range(0,len(al)):\n",
    "                active_listing = active_listing +al[x]\n",
    "        else:\n",
    "            active_listing = al[0]\n",
    "        active_listing = int(active_listing)\n",
    "        ACTIVE_LISTING.append(active_listing)\n",
    "        #print(active_listing)\n",
    "        \n",
    "        company_address = soup.find_all(\"span\",{\"class\":\"table__column\"})[3].text\n",
    "        company_address = company_address.replace(\",\",\" ;\")\n",
    "        company_address = company_address.replace(\"\\r\",\" \")\n",
    "        company_address = company_address.replace(\"\\n\",\" ;\")\n",
    "        company_address =\"'\"+ company_address+\"'\"\n",
    "        COMPANY_ADDRESS.append(company_address)\n",
    "        #print(company_address) \n",
    "        \n",
    "        about=soup.find_all(\"p\",{\"data-qs\":\"text-trimmer\"})\n",
    "        for x in range (0,len(about)):\n",
    "            description = about[x].text\n",
    "            if len(description) == 0:\n",
    "                description = \"NULL\"\n",
    "            else: \n",
    "                description = description.replace(\"\\r\",\" \")\n",
    "                description = description.replace(\"\\n\",\" \")\n",
    "                #description = description.split(\"\\n\")\n",
    "                description = description.replace(\",\",\" ;\")\n",
    "                description =\"'\"+ description+\"'\"\n",
    "                #print(description)\n",
    "            \n",
    "            COMPANY_ABOUT.append(description)\n",
    "            #print (description)\n",
    "        \n",
    "        broker_number = soup.find_all(\"span\", {\"class\": \"button__text button__text-value button__phone-ltr button__text--is-hidden\"})[0].text\n",
    "        broker_number =\"'\"+ broker_number+\"'\"\n",
    "        COMPANY_NUMBERS.append(broker_number)\n",
    "        #print(broker_number)\n",
    "        \n",
    "        \n",
    "    company_data=[] #{\"Company names\",\"numbers\"}\n",
    "    for x in range(0,len(COMPANY_NUMBERS)):\n",
    "            company_data.append([COMPANY_NAMES[x],COMPANY_NUMBERS[x],ACTIVE_LISTING[x],COMPANY_ADDRESS[x],COMPANY_ABOUT[x]])\n",
    "    df = pd.DataFrame(company_data)\n",
    "    df.to_csv(\"company_test.csv\",mode=\"a\",sep=',', encoding='utf-8', index=False, header=False)\n",
    "    \n",
    "    #print(page)\n",
    "    page = page +1\n",
    "    \n",
    "    COMPANY_NAMES.clear()\n",
    "    ACTIVE_LISTING.clear()\n",
    "    COMPANY_ABOUT.clear()\n",
    "    COMPANY_ADDRESS.clear()\n",
    "    COMPANY_NUMBERS.clear()\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
